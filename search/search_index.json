{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Neurobagel","text":""},{"location":"#preparing-the-data","title":"Preparing the data","text":"<p>To use the Neurobagel Annotation tool,  please prepare your tabular data as a single file.</p> <p>Here are some examples:</p>"},{"location":"#a-bids-participantstsv-file","title":"A BIDS participants.tsv file","text":"<p>If you have a BIDS compliant participants.tsv file that contains  all of your demographic information,  then you can annotate this file with the Neurobagel Annotator to create a new data dictionary. </p> <p>For example:</p> participant_id age sex tools sub-01 22 female WASI-2 sub-02 28 male Stroop ..."},{"location":"#a-multi-session-file","title":"A multi-session file","text":"<p>If you have multi-session tabular data (e.g. different ages for different sessions), then you should combine all information into a single tabular file.</p> <p>Note</p> <p>A participants.tsv file with multiple sessions is not BIDS compliant. If you want to store a multi-session file in a BIDS dataset, you could do so in the <code>/pheno</code> subdirectory.</p> <p>For example:</p> participant_id session_id age tools sub-01 ses-01 22 WASI-2 sub-01 ses-02 23 sub-02 ses-01 28 Stroop ..."},{"location":"#multiple-participant-or-session-ids","title":"Multiple participant or session IDs","text":"<p>In some cases there may be a need for more than one set of IDs  for participants and/or sessions. For example if a participant was first enrolled in a behavioural study with one type of IDs  and later joined an imaging study with different IDs. In such a case, you should include both participant IDs in the tabular file. The only requirement is that the combination of IDs has to be unique.</p> <p>For example, this would not be allowed:</p> participant_id alternative_participant_id ... sub-01 SID-1234 sub-01 SID-2222 sub-02 SID-1234 <p>The same rules apply to session IDs. </p> <p>For example:</p> participant_id alt_participant_id session_id alt_session_id age ... sub-01 SID-1234 ses-01 visit-1 22 sub-01 SID-1234 ses-02 visit-2 23 sub-02 SID-2222 ses-01 visit-1 28 ..."},{"location":"datamodels/","title":"Neurobagel Data Models","text":""},{"location":"datamodels/#data-dictionaries","title":"Data Dictionaries","text":"<p>When you annotate a demographic <code>.csv</code> file with the Neurobagel Annotation tool, your annotations are stored in a data dictionary. Neurobagel uses a structure for these data dictionaries that is compatible  with and expands on the  BIDS <code>participant.json</code> data dictionaries. </p> <p>Info</p> <p>The specification for how a Neurobagel data dictionary is structured is also called a schema.  Because Neurobagel data dictionaries are stored as <code>.json</code> files, we use the <code>jsonschema</code> schema language  to write the specification.</p> <p>The Neurobagel data dictionaries add a new <code>Annotations</code> attribute  to each column entry to store the semantic annotations.</p> <p>Here is an example of a BIDS data dictionary:</p> <pre><code>{\n\"age\": {\n\"Description\": \"age of the participant\",\n\"Units\": \"years\"\n},\n\"sex\": {\n\"Description\": \"sex of the participant as reported by the participant\",\n\"Levels\": {\n\"M\": \"male\",\n\"F\": \"female\"\n}\n}\n}\n</code></pre> <p>And here is the same example with Neurobagel annotations added:</p> <pre><code>{\n\"age\": {\n\"Description\": \"age of the participant\",\n\"Units\": \"years\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"http://neurobagel.org/vocab/Age\",\n\"Label\": \"Age\"\n},\n\"Transformation\": {\n\"TermURL\": \"http://neurobagel.org/vocab/int\",\n\"Label\": \"Integer\"\n}\n}\n},\n\"sex\": {\n\"Description\": \"sex of the participant as reported by the participant\",\n\"Levels\": {\n\"M\": \"male\",\n\"F\": \"female\"\n},\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"http://neurobagel.org/vocab/Sex\",\n\"Label\": \"Sex\"\n},\n\"Levels\": {\n\"M\": {\n\"TermURL\": \"http://purl.bioontology.org/ontology/SNOMEDCT/248153007\",\n\"Label\": \"Male\"\n},\n\"F\": {\n\"TermURL\": \"http://purl.bioontology.org/ontology/SNOMEDCT/248152002\",\n\"Label\": \"Female\"\n}\n},\n\"MissingValues\": [\n\"\",\n\" \"\n]\n}\n}\n}\n</code></pre> <p>Note that we use a Neurobagel namespace (URI: http://neurobagel.org/vocab/) for controlled terms representing classes or properties that we model, such as <code>\"Age\"</code> and <code>\"Sex\"</code>, but that these can have equivalent terms in another namespace we are using. For example, the following terms from the Neurobagel annotations above are conceptually equivalent to terms from the SNOMED-CT namespace:</p> Neurobagel namespace term External controlled vocabulary term http://neurobagel.org/vocab/Age http://purl.bioontology.org/ontology/SNOMEDCT/397669002 http://neurobagel.org/vocab/Sex http://purl.bioontology.org/ontology/SNOMEDCT/184100006"},{"location":"dictionaries/","title":"Neurobagel Data Dictionaries","text":"<p>The Neurobagel annotator creates a BIDS compatible data dictionary and augments the information that BIDS recommends with  unambiguous semantic tags.</p> <p>Below we'll outline several special cases using the following example <code>participants.tsv</code> file:</p> participant_id session_id group age sex updrs_1 updrs_2 sub-01 ses-01 PAT 25 M 2 sub-01 ses-02 PAT 26 M 3 5 sub-02 ses-01 CTL 28 F 1 1 sub-02 ses-02 CTL 29 F 1 1 <p>Controlled terms in the below examples are shortened using the RDF prefix/context syntax for json-ld:</p> <pre><code>{\n\"@context\": {\n\"nb\": \"http://neurobagel.org/vocab/\",\n\"purl\": \"http://purl.obolibrary.org/obo/\",\n\"snomed\": \"http://purl.bioontology.org/ontology/SNOMEDCT/\",\n\"cogatlas\": \"https://www.cognitiveatlas.org/task/id/\"\n}\n}\n</code></pre>"},{"location":"dictionaries/#participant-identifier","title":"Participant identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n\"participant_id\": {\n\"Description\": \"A participant ID\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:ParticipantID\",\n\"Label\": \"Subject Unique Identifier\"\n}\n}\n}\n}\n</code></pre> <p>Note</p> <p><code>participant_id</code> is a reserved name in BIDS and BIDS data dictionaries therefore typically don't annotate this column. Neurobagel supports multiple subject ID columns for situations where a study is using more than one ID scheme.</p>"},{"location":"dictionaries/#session-identifier","title":"Session identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n\"session_id\": {\n\"Description\": \"A session ID\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:SessionID\",\n\"Label\": \"Run Identifier\"\n}\n}\n}\n}\n</code></pre> <p>Note</p> <p>Unlike the BIDS specification, Neurobagel supports a <code>participants.tsv</code> file with a <code>session_id</code> field.</p>"},{"location":"dictionaries/#diagnosis","title":"Diagnosis","text":"<p>Terms from the SNOMED-CT ontology for clinical diagnosis. Terms from the National Cancer Institute Thesaurus for healthy control status.</p> <pre><code>{\n\"group\": {\n\"Description\": \"Group variable\",\n\"Levels\": {\n\"PD\": \"Parkinson's patient\",\n\"CTRL\": \"Control subject\",\n},\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Diagnosis\",\n\"Label\": \"Diagnosis\"\n},\n\"Levels\": {\n\"PD\": {\n\"TermURL\": \"snomed:49049000\",\n\"Label\": \"Parkinson's disease\"\n},\n\"CTRL\": {\n\"TermURL\": \"purl:NCIT_C94342\",\n\"Label\": \"Healthy Control\"\n}\n}\n}\n}\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a term from the Neurobagel namespace because <code>\"Diagnosis\"</code> is a standardized term.</p>"},{"location":"dictionaries/#sex","title":"Sex","text":"<p>Terms are from the SNOMED-CT ontology, which has controlled terms aligning with BIDS <code>participants.tsv</code> descriptions for sex.</p> <pre><code>{\n\"sex\": {\n\"Description\": \"Sex variable\",\n\"Levels\": {\n\"M\": \"Male\",\n\"F\": \"Female\"\n},\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Sex\",\n\"Label\": \"Sex\"\n},\n\"Levels\": {\n\"M\": {\n\"TermURL\": \"snomed:248153007\",\n\"Label\": \"Male\"\n},\n\"F\": {\n\"TermURL\": \"snomed:248152002\",\n\"Label\": \"Female\"\n}\n}\n}\n}\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a Neurobagel scoped term for <code>\"Sex\"</code> because  this is a Neurobagel common data element.</p>"},{"location":"dictionaries/#age","title":"Age","text":"<p>Neurobagel has a common data element for <code>\"Age\"</code> which describes a continuous column. To ensure age values are represented as floats in Neurobagel graphs, Neurobagel encodes the relevant \"heuristic\" describing the value format of a given age column. This heuristic, stored in the <code>Transformation</code> annotation, corresponds internally to a specific transformation that is used to convert the values to float ages.</p> <p>Possible heuristics: </p> TermURL Label nb:float float value nb:int integer value nb:euro european decimal value nb:bounded bounded value nb:iso8061 period of time defined according to the ISO8601 standard <pre><code>{\n\"age\": {\n\"Description\": \"Participant age\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Age\",\n\"Label\": \"Chronological age\"\n},\n\"Transformation\": {\n\"TermURL\": \"nb:euro\",\n\"Label\": \"European value decimals\"\n}\n}\n}\n}\n</code></pre>"},{"location":"dictionaries/#assessment-tool","title":"Assessment tool","text":"<p>For assessment tools like cognitive tests or rating scales,  Neurobagel encodes whether the tool was successfully completed. Because assessment tools often have several subscales or items  that can be stored as separate columns in the tabular <code>participant.tsv</code> file, each assessment tool column gets at least two annotations:</p> <ul> <li>one to classify it as <code>IsAbout</code> the generic category of assessment tools</li> <li>one to classify it as <code>PartOf</code> the specific assessment tool</li> </ul> <p>An additional annotation <code>MissingValues</code> can be used to specify value(s) in an assessment tool column which represent that the participant is missing a value/response for that subscale, when instances of missing values are present.</p> <pre><code>{\n\"updrs_1\": {\n\"Description\": \"item 1 scores for UPDRS\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Assessment\",\n\"Label\": \"Assessment tool\"\n},\n\"IsPartOf\": {\n\"TermURL\": \"cogatlas:tsk_4a57abb949ece\",\n\"Label\": \"Unified Parkinson's Disease Rating Scale\"\n}\n}\n},\n\"updrs_2\": {\n\"Description\": \"item 2 scores for UPDRS\",\n\"Annotations\": {\n\"IsAbout\": {\n\"TermURL\": \"nb:Assessment\",\n\"Label\": \"Assessment tool\"\n},\n\"IsPartOf\": {\n\"TermURL\": \"cogatlas:tsk_4a57abb949ece\",\n\"Label\": \"Unified Parkinson's Disease Rating Scale\"\n},\n\"MissingValues\": [\"\"]\n}\n}\n}\n</code></pre> <p>To determine whether a specific assessment tool is available for a given participant, we then combine all of the columns that were classified as <code>PartOf</code> that specific tool and then apply a simple <code>all()</code> heuristic to check that none of the columns contain any <code>MissingValues</code>.</p> <p>For the above example, this would be:</p> particpant_id updrs_1 updrs_2 sub-01 2 sub-02 1 1 <p>Therefore: </p> particpant_id updrs_available sub-01 False sub-02 True"},{"location":"start/","title":"SysAdmin","text":"<p>These instructions are for a sysadmin looking to deploy Neurobagel locally in an institute or lab.</p>"},{"location":"start/#ecosystem","title":"Ecosystem","text":"<p>The Neurobagel ecosystem consists of four tools:</p> <ul> <li>the annotation tool <ul> <li>to create harmonized annotations of phenotypic data</li> <li>intended for use by researchers and domain experts</li> <li>static site, deployed on Github Pages</li> <li>annotate.neurobagel.org</li> </ul> </li> <li>the CommandLineInterface <ul> <li>to extract metadata from annotated phenotypic and BIDS data</li> <li>intended for data managers to create graph ready data</li> <li>neurobagel/bagel-cli</li> </ul> </li> <li>the graph and API<ul> <li>to store and query extracted metadata</li> <li>intended for platform owners and for isolated deployments</li> <li>api.neurobagel.org</li> </ul> </li> <li>The query tool<ul> <li>to create cohort queries and display results</li> <li>intended for use by researchers and data consumers</li> <li>static site, deployed on Github Pages</li> <li>query.neurobagel.org</li> </ul> </li> </ul> Todo <p>Add Neurobagel figure for overview.</p>"},{"location":"start/#get-a-license-for-stardog","title":"Get a license for Stardog","text":"<p>We use Stardog as our Graph store application.  Stardog has a free, annually renewable license for academic use. In order to make a separate deployment of Neurobagel,  you should therefore first request your own Stardog license. You can request a Stardog license here:</p> <p>https://www.stardog.com/license-request/</p> <p>Don't pick the wrong license</p> <p>Stardog is a company that offers their graph store solutions both as a self-hosted, downloadable tool (what we want) and as a cloud hosted subscription model (what we do not want). Both tiers offer free access and the website has a tendency to steer you towards the cloud offering. Make sure you request a license key for Stardog.</p> <p></p> <p>The Stardog license is typically automatically granted via email in 24 hours. </p> <p>The license you receive will be a downloadable file.  It is valid for one year and for a major version of Stardog. You will need to download the license in a place that is accessible to your new Stardog instance when it is launched (see below).</p>"},{"location":"start/#launch-the-api-and-graph-stack","title":"Launch the API and Graph stack","text":"<p>Please follow the instructions here  to pull the API and Stardog Docker images and then launch both via <code>docker compose</code> (Option 1).</p> <p>Your license file has to be in the <code>STARDOG_HOME</code> directory.</p> Ensure that shell variables do not clash with <code>.env</code> file <p>When you follow the setup docs for the API, make sure to explicitly set the following variables:</p> <ul> <li><code>USERNAME</code></li> <li><code>PASSWORD</code></li> <li><code>GRAPH_DB</code></li> <li><code>STARDOG_ROOT</code></li> </ul> <p>If the shell you run <code>docker compose</code> from already has any shell variable of the same name set,  the shell variable will take precedence over the configuration of <code>.env</code>!  Either unset the local variable or export the <code>.env</code> file first.</p> <p>See also the docs.</p>"},{"location":"start/#setup-for-the-first-run","title":"Setup for the first run","text":"<p>When you launch the Stardog graph for the first time, there are a couple of setup steps that need to be done.  These will not have to be repeated for subsequent starts.</p> <p>To intereact with the Stardog graph,  you have two general options:</p> <ol> <li>Send HTTP request against the HTTP API of the Stardog graph instance (e.g. with <code>curl</code>). See https://stardog-union.github.io/http-docs/ for a full reference of API endpoints</li> <li>Use the free Stardog-Studio web app. See the Stardog documention for instruction to deploy Stardog-Studio as a Docker container.</li> </ol> <p>Info</p> <p>Stardog-Studio is the most accessible way  of manually interacting with a Stardog instance.  Here we will focus instead on using the HTTP API for configuration, as this allows programmatic access. All of these steps can also be achieved via Stardog-Studio manually. Please refer to the  official docs to learn how.</p>"},{"location":"start/#change-the-superuser-password","title":"Change the superuser password","text":"<p>When you first launch Stardog,  a default <code>admin</code> user with superuser privilege will automatically be created for you. You should first change the password of this user:</p> <pre><code>curl -X PUT -i -u \"admin:admin\" http://localhost:5820/admin/users/admin/pwd \\\n--data '{\"password\": \"NewPassword\"}'\n</code></pre>"},{"location":"start/#create-a-new-user","title":"Create a new user","text":"<p>The <code>.env</code> file created as part of the <code>docker compose</code> setup instructions declares the <code>USERNAME</code> and <code>PASSWORD</code> for the API user. The API will send requests to the graph using these credentials. When you launch Stardog for the first time,  we have to create this user:</p> <pre><code>curl -X POST -i -u \"admin:NewPassword\" http://localhost:5820/admin/users \\\n-H 'Content-Type: application/json' \\\n--data '{\n    \"username\": \"NewUser\",\n    \"password\": [\n        \"NewUserPassword\"\n    ]\n}'\n</code></pre> <p>Confirm that the new user exists:</p> <pre><code>curl -u \"admin:NewPassword\" http://localhost:5820/admin/users\n</code></pre> <p>Note</p> <p>Make sure to use the exact <code>USERNAME</code> and <code>PASSWORD</code> you defined in the <code>.env</code> file when creating the new user. Otherwise the API will not have the correct permission to query the graph.</p>"},{"location":"start/#create-new-database","title":"Create new database","text":"<p>When you first launch Stardog, there are no graph databases. You have to create a new one to store your metadata.</p> <p>If you have defined a custom <code>GRAPH_DB</code> name in the <code>.env</code> file, make sure to create a database with a matching name. By default the API will query a graph database with a name of <code>test_data</code>.</p> <pre><code>curl -X POST -i -u \"admin:NewPassword\" http://localhost:5820/admin/databases \\\n--form 'root=\"{\\\"dbname\\\":\\\"test_data\\\"}\"'\n</code></pre> <p>Now we need to give our new user read and write permission for  this database:</p> <pre><code>curl -X PUT -i -u \"admin:NewPassword\" http://localhost:5820/admin/permissions/user/NewUser \\\n-H 'Content-Type: application/json' \\\n--data '{\n    \"action\": \"ALL\",\n    \"resource_type\": \"DB\",\n    \"resource\": [\n        \"test_data\"\n    ]\n}'\n</code></pre> Finer permission control is also possible <p>For simplicity's sake, here we give <code>\"ALL\"</code> permission to the user. The Stardog API provide more fine grained permission control. See the official API documentation.</p>"},{"location":"start/#add-some-test-data","title":"Add some test data","text":"<p>In order to test that the setup has worked correctly, we need to add some data to the database.</p> <p>You can take two example files from the Neurobagel example repository to get started:</p> <ul> <li>example 1</li> <li>example 2</li> </ul> <p>Normally you would create these files by first annotating the phenotypic information of a BIDS dataset with the  Neurobagel annotator, and then parsing the annotated BIDS dataset with the Neurobagel CLI.</p> <p>Upload the example files to the graph using this command:</p> <pre><code>curl -u \"admin:NewPassword\" -i -X POST http://localhost:5820/test_data \\\n-H \"Content-Type: text/turtle\" \\\n--data-binary @example_1.ttl\n</code></pre>"},{"location":"start/#test-the-new-deployment","title":"Test the new deployment","text":"<p>You can run a test query against the API:</p> <pre><code>curl -X 'GET' \\\n  'http://localhost:8000/query/' \\\n  -H 'accept: application/json'\n</code></pre> <p>or directly use the interactive documentation of the API.</p>"},{"location":"term_naming_standards/","title":"Neurobagel standards for controlled term naming","text":""},{"location":"term_naming_standards/#naming-conventions","title":"Naming conventions","text":""},{"location":"term_naming_standards/#namespace-prefixes","title":"Namespace prefixes","text":"<ul> <li>Names should be all lowercase (e.g., <code>nidm</code>, <code>cogatlas</code>)</li> </ul>"},{"location":"term_naming_standards/#properties-graph-edges","title":"Properties (graph \"edges\")","text":"<ul> <li>Names should adhere to camelCase (uses capitalized words except for the first word/letter)</li> <li>Should be a compound of:<ul> <li>a verb relevant to the property (e.g., hasAge, isSubjectGroup)</li> <li>the range of the property, (e.g.,hasDiagnosis points to a Diagnosis object)</li> </ul> </li> </ul> <p>What this might look like in semantic triples: <pre><code>&lt;Subject&gt; &lt;nb:hasDiagnosis&gt; &lt;snomed:1234&gt;\n&lt;snomed:1234&gt; &lt;rdf:type&gt; &lt;nb:Diagnosis&gt;\n</code></pre></p>"},{"location":"term_naming_standards/#classes-or-resources-graph-nodes","title":"Classes or resources (graph \"nodes\")","text":"<ul> <li>Names should adhere to PascalCase (each word capitalized)</li> <li>Where possible, simplify to a single word (e.g., <code>Diagnosis</code>, <code>Dataset</code>, <code>Sex</code>)</li> </ul> <p>Note</p> <p>Generally, we own the terms for properties and classes (e.g., Diagnosis, Assessment) but not the resources representing instances of classes such as specific diagnosis, sex, or assessment values (these are reused from existing vocabularies).</p> <p>In cases where we reuse a term for a class that comes from an existing controlled vocabulary, and that vocabulary follows a different naming convention (e.g., all lowercase), we should follow the existing naming convention.</p>"},{"location":"term_naming_standards/#currently-used-namespaces","title":"Currently used namespaces","text":"Prefix IRI Types of terms <code>nb</code> http://neurobagel.org/vocab/ Neurobagel-\"owned\" properties and classes <code>nbg</code> http://neurobagel.org/graph/ Neurobagel graph databases <code>snomed</code> http://purl.bioontology.org/ontology/SNOMEDCT/ diagnosis and sex values <code>nidm</code> http://purl.org/nidash/nidm# imaging modalities <code>cogatlas</code> https://www.cognitiveatlas.org/task/id/ cognitive assessments and tasks"},{"location":"term_naming_standards/#what-if-an-nb-term-already-exists-in-another-controlled-vocabulary","title":"What if an <code>nb</code> term already exists in another controlled vocabulary?","text":"<p>If there is an equivalent controlled term to one we are defining in a different namespace,  we document this and express their equivalence using <code>owl:sameAs</code>.</p> <p>Example: If our term is <code>nb:Subject</code> and <code>nidm:Subject</code> is conceptually equivalent: <pre><code>&lt;nb:12345&gt; a &lt;nb:Subject&gt;\n&lt;nb:Subject&gt; a &lt;rdfs:Resource&gt;;\n    &lt;owl:sameAs&gt; &lt;nidm:Subject&gt;\n</code></pre></p>"},{"location":"term_naming_standards/#other-general-guidelines","title":"Other general guidelines","text":"<ul> <li>Each property (edge) should use a single namespace for the resources it corresponds to</li> <li>Where possible, hardcode or refer to identifiers and not human-readable labels</li> </ul>"},{"location":"mr_proc/code_org/","title":"Code organization","text":""},{"location":"mr_proc/code_org/#code-organization","title":"Code organization","text":"<p>mr_proc codebase is divided into data processing <code>workflows</code> and data availability <code>trackers</code>: </p> <ol> <li>workflow:<ul> <li>MRI data<ul> <li>Custom script to organize raw dicoms (i.e. scanner output) into a flat participant-level directory. </li> <li>Convert dicoms into BIDS using Heudiconv</li> <li>Runs a set of containerized MRI image processing pipelines </li> </ul> </li> <li>Tabular data<ul> <li>Custom scripts to organize raw tabular data (e.g. clinial assessments)</li> <li>Custom scripts to normalize and standardize data and metadata for downstream harmonization (see NeuroBagel)</li> </ul> </li> </ul> </li> <li>trackers:<ul> <li>Tracks available raw, standardized, and processed data</li> <li>Generates <code>bagels</code> for NeuorBagel graph and dashboard. </li> </ul> </li> </ol>"},{"location":"mr_proc/code_org/#legend","title":"legend","text":"<ul> <li>Red: dataset specific code and config</li> <li>Yellow: NeuroBagel interface</li> </ul>"},{"location":"mr_proc/configs/","title":"Configs","text":""},{"location":"mr_proc/configs/#global-files","title":"Global files","text":"<p>mr_proc consists of two global files for specifying local data and container paths and recruitment manifest </p>"},{"location":"mr_proc/configs/#global-configs-global_configsjson","title":"Global configs: <code>global_configs.json</code>","text":"<ul> <li>This is a dataset specific file and needs to be modified based on local configs and paths</li> <li>This file is used as an input to all workflow <code>run scripts</code> to read, process and track available data</li> <li>Copy, rename, and populate sample_global_configs.json </li> <li>This file contains:<ul> <li>Path to mr_proc_dataset</li> <li>List of pipelines + versions</li> <li>Path to local <code>container_store</code> comprising containers used by several workflow scripts</li> </ul> </li> </ul> <p>Suggestion</p> <p>Although not mandatory, for consistency the preferred location would be: <code>&lt;DATASET_ROOT&gt;/proc/global_configs.json</code>.</p>"},{"location":"mr_proc/configs/#sample-global_configsjson","title":"Sample <code>global_configs.json</code>","text":"<pre><code>{\n\"DATASET_NAME\": \"MyDataset\",\n\"DATASET_ROOT\": \"/path/to/MyDataset\",\n\"CONTAINER_STORE\": \"/path/to/container_store\",\n\"SINGULARITY_PATH\": \"singularity\",\n\"TEMPLATEFLOW_DIR\": \"/path/to/templateflow\",\n\n\"BIDS\": {\n\"heudiconv\": {\n\"VERSION\": \"0.11.6\",    \"CONTAINER\": \"heudiconv_{}.sif\",\n\"URL\": \"\"\n},\n\"validator\":{\n\"CONTAINER\": \"bids_validator.sif\",\n\"URL\": \"\"\n\n}\n},\n\n\"PROC_PIPELINES\": {\n\"mriqc\": {\n\"VERSION\": \"\",\n\"CONTAINER\": \"mriqc_{}.sif\",\n\"URL\": \"\"\n},\n\"fmriprep\": {\n\"VERSION\": \"20.2.7\",\n\"CONTAINER\": \"fmriprep_{}.sif\",\n\"URL\": \"\"\n},\n\"freesurfer\": {\n\"VERSION\": \"6.0.1\",\n\"CONTAINER\": \"fmriprep_{}.sif\",\n\"URL\": \"\"\n}\n}\n}\n</code></pre>"},{"location":"mr_proc/configs/#participant-manifest-mr_proc_manifestcsv","title":"Participant manifest: <code>mr_proc_manifest.csv</code>","text":"<ul> <li>This list serves as the ground-truth for subject and visit (i.e. session) availability</li> <li>Create the <code>mr_proc_manifest.csv</code> in <code>&lt;DATASET_ROOT&gt;/tabular/</code> comprising following columns<ul> <li><code>participant_id</code>: ID assigned during recruitment (at times used interchangeably with subject_id)</li> <li><code>participant_dicom_dir</code>: participant-level dicom directory name on the disk</li> <li><code>visit</code>: label to denote participant visit for data acquisition (e.g. \"baseline\", \"m12\", \"m24\" or \"V01\", \"V02\" etc.)</li> <li><code>session</code>: alternative naming for visit - typically used for imaging data to comply with BIDS standard</li> <li><code>datatype</code>: a list of acquired imaging datatype as defined by BIDS standard</li> <li><code>bids_id</code>: this is created automatically which attaches <code>sub-</code> prefix and removes any non-alphanumeric chacaters (e.g. \"-\" or \"_\") from the original <code>participant_id</code> string. <code>participant_id</code> and <code>bids_id</code> in <code>mr_proc_manifest.csv</code> are used to link tabular and MRI data</li> </ul> </li> <li>New participant are appended upon recruitment as new rows</li> <li>Participants with multiple visits (i.e. sessions) should be added as separate rows</li> </ul>"},{"location":"mr_proc/configs/#sample-mr_proc_manifestcsv","title":"Sample <code>mr_proc_manifest.csv</code>","text":"participant_id participant_dicom_dir visit session datatype bids_id 001 MyStudy_001_2021 V01 ses-01 [\"anat\",\"dwi\",\"fmap\",\"func\"] sub-001 001 MyStudy_001_2022 V02 ses-02 [\"anat\"] sub-001 002 MyStudy_002_2021 V01 ses-01 [\"anat\",\"dwi\"] sub-002 002 MyStudy_002_2024 V03 ses-03 [\"anat\",\"dwi\"] sub-002"},{"location":"mr_proc/data_org/","title":"Data organization","text":""},{"location":"mr_proc/data_org/#data-organization","title":"Data organization","text":"<p>mr_proc dataset consists of a specific directory structure to organize MRI and tabular data</p> <p>Directories: </p> <ul> <li><code>tabular</code><ul> <li><code>demographics</code>: contains <code>mr_proc_manifest.csv</code></li> <li><code>assessments</code>: contains clinical assessments (e.g. MoCA) </li> </ul> </li> <li><code>downloads</code>: data dumps from remote data-stores (e.g. LONI)</li> <li><code>scratch</code>: space for un-organized data and wrangling</li> <li><code>dicom</code>: participant-level dicom dirs</li> <li><code>bids</code>: BIDS formatted dataset</li> <li><code>derivatives</code>: output of processing pipelines (e.g. fmriprep, mriqc)</li> <li><code>proc</code>: space for config and log files of the processing pipelines</li> <li><code>backups</code>: data backup space (tars)</li> <li><code>releases</code>: data releases (symlinks)</li> </ul> <p></p>"},{"location":"mr_proc/installation/","title":"Installation","text":""},{"location":"mr_proc/installation/#code-installation-and-dataset-setup","title":"Code installation and dataset setup","text":"<p>mr_proc workflow comprises mr_proc codebase that operates on mr_proc dataset with a specific directory structure, which is initialized with a tree.py script. </p>"},{"location":"mr_proc/installation/#mr_proc-codeenv-installation","title":"mr_proc code+env installation","text":"<ul> <li>Change dir to where you want to clone this repo, e.g.: <code>cd /home/&lt;user&gt;/projects/&lt;my_project&gt;/code/</code></li> <li>Create a new venv: <code>python3 -m venv mr_proc_env</code> </li> <li>Activate your env: <code>source mr_proc_env/bin/activate</code> </li> <li>Clone this repo: <code>git clone https://github.com/neurodatascience/mr_proc.git</code></li> <li>Change dir to <code>mr_proc</code> </li> <li>Install python dependencies: <code>pip install -e .</code> </li> </ul>"},{"location":"mr_proc/installation/#mr_proc-dataset-directory-setup","title":"mr_proc dataset directory setup","text":"<ul> <li>Run <code>python tree.py</code> to create mr_proc dataset directory tree</li> </ul> <p>Sample cmd: <pre><code>python tree.py --mr_proc_root &lt;DATASET_ROOT&gt;\n</code></pre></p> <ul> <li><code>data_disk</code>: data storage location on a local disk</li> <li><code>DATASET_ROOT</code>: root (starting point) of the mr_proc structured dataset</li> </ul> <p>Suggestion</p> <p>We suggest naming DATASET_ROOT directory after a study or a cohort. </p>"},{"location":"mr_proc/overview/","title":"Overview","text":""},{"location":"mr_proc/overview/#what-is-mr_proc","title":"What is mr_proc?","text":"<p>Process long and prosper</p> <p>mr_proc is a workflow manager for:</p> <ol> <li>MRI and tabular data curation</li> <li>Standardized processing </li> <li>Raw + processed data tracking</li> </ol>"},{"location":"mr_proc/overview/#modules","title":"Modules","text":"<ol> <li><code>Code</code>: Codebase repo for running and tracking workflows</li> <li><code>Data</code>: Dataset organized in a specific directory structure</li> <li><code>Containers</code>: Singularity containers encapsulating your processing pipelines</li> </ol>"},{"location":"mr_proc/overview/#objectives","title":"Objectives","text":"<ol> <li>Standardized data i.e. convert DICOMs into BIDS</li> <li>Run commonly used image processing pipelines e.g. FreeSurfer, fMRIPrep</li> <li>Organize processed MR data inside <code>derivatives</code> directory</li> <li>Organize demographic and clinical assessment data inside <code>tabular</code> directory</li> <li>Run tracker scripts to populate <code>bagel.csv</code> with tabular and processing pipeline metadata</li> <li>Provide metadata to <code>NeuroBagel</code> to allow dashboarding and querying participants across multiple studies</li> </ol>"},{"location":"mr_proc/overview/#organization","title":"Organization","text":"<p>Organization of <code>Code</code>, <code>Data</code>, and <code>Container</code> modules</p> <p></p>"},{"location":"mr_proc/overview/#steps","title":"Steps","text":"<p>The mr_proc workflow steps and linked identifiers (i.e. participant_id, dicom_id, bids_id) are show below:</p> <p></p>"},{"location":"mr_proc/workflow/bids_conv/","title":"BIDS conversion","text":""},{"location":"mr_proc/workflow/bids_conv/#objective","title":"Objective","text":"<p>Convert DICOMs to BIDS using Heudiconv (tutorial). </p>"},{"location":"mr_proc/workflow/bids_conv/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/dicom</code></li> <li><code>&lt;DATASET_ROOT&gt;/bids</code></li> <li><code>heuristic.py</code></li> </ul>"},{"location":"mr_proc/workflow/bids_conv/#procedure","title":"Procedure","text":"<ul> <li>Ensure you have the appropriate HeuDiConv container listed in your <code>global_configs.json</code></li> <li>Use run_bids_conv.py to run HeuDiConv <code>stage_1</code> and <code>stage_2</code>.  </li> <li>Run <code>stage_1</code> to generate a list of available protocols from the DICOM header. These protocols are listed in <code>&lt;DATASET_ROOT&gt;/bids/.heudiconv/&lt;participant_id&gt;/info/dicominfo_ses-&lt;session_id&gt;.tsv</code></li> </ul> <p>Sample cmd: <pre><code>python run_bids_conv.py \\\n--global_config &lt;global_config_file&gt; \\\n--session_id &lt;session_id&gt; \\\n--stage 1\n</code></pre></p> <p>Note</p> <p>If participants have multiple sessions (or visits), these need to be converted separately and combined post-hoc to avoid Heudiconv errors. </p> <ul> <li>Copy+Rename sample_heuristic.py to <code>heuristic.py</code> in the code repo itself. Then edit <code>./heuristic.py</code> to create a name-mapping (i.e. dictionary) for BIDS organization based on the list of available protocols. </li> </ul> <p>Note</p> <p>This file automatically gets copied into <code>&lt;DATASET_ROOT&gt;/proc/heuristic.py</code> to be seen by the Singularity container.</p> <ul> <li>Run <code>stage_2</code> to convert the dicoms into BIDS format based on the mapping from <code>heuristic.py</code>. </li> </ul> <p>Sample cmd: <pre><code>python run_bids_conv.py \\\n   --global_config &lt;global_config_file&gt; \\\n   --session_id &lt;session_id&gt; \\\n   --stage 2\n</code></pre></p> <p>Note</p> <p>Once <code>heuristic.py</code> is finalized, only <code>stage_2</code> needs to be run peridodically unless new scan protocol is added.</p>"},{"location":"mr_proc/workflow/bids_conv/#bids-validator","title":"BIDS validator","text":"<ul> <li>Make sure you have the appropriate bids_validator container in your <code>&lt;DATASET_ROOT&gt;/proc/global_configs.json</code></li> <li>Use run_bids_val.sh to check for errors and warnings<ul> <li>Sample command: <code>run_bids_val.sh &lt;bids_dir&gt; &lt;log_dir&gt;</code> </li> <li>Alternatively if your machine has a browser you can also use an online validator</li> </ul> </li> </ul> <p>Note</p> <p>Make sure you match the version of Heudiconv and BIDS validator standard</p> <p>Note</p> <p>Heuristic file needs to be updated if your dataset has different protocols for different participants</p>"},{"location":"mr_proc/workflow/bids_conv/#fix-heudiconv-errors","title":"Fix HeuDiConv errors","text":"<ul> <li>If you see errors from BIDS validator, it is possible that HeuDiConv may not be supporting your MRI sequence. In that case add a function to fix_heudiconv_issues.py to manually rename files, and run the script posthoc. </li> <li>Make sure to open an issue on HeuDiConv Github for fix in future release. </li> </ul>"},{"location":"mr_proc/workflow/dicom_org/","title":"DICOM organization","text":""},{"location":"mr_proc/workflow/dicom_org/#objective","title":"Objective","text":"<p>This is a dataset specific process and needs to be customized based on local scanner DICOM dumps and file naming. This organization should produce, for a given session, participant specific dicom dirs. Each of these participant-dir contains a flat list of dicoms for the participant for all available imaging modalities and scan protocols.</p>"},{"location":"mr_proc/workflow/dicom_org/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/downloads</code></li> <li><code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms</code></li> <li><code>&lt;DATASET_ROOT&gt;/dicom</code></li> <li><code>&lt;DATASET_ROOT&gt;/tabular/demographics/mr_proc_manifest.csv</code></li> </ul>"},{"location":"mr_proc/workflow/dicom_org/#procedure","title":"Procedure","text":"<ul> <li>Download DICOM dumps (e.g. ZIPs / tarballs) in the <code>&lt;DATASET_ROOT&gt;/downloads</code> directory. It is recommended that different visits (i.e. sessions) are downloaded in separate sub-directories and named as listed in the <code>global_config.json</code>.</li> <li>Extract (and rename* if needed) all participants into <code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms</code> separately for each visit (i.e. session). </li> </ul> <p>Note</p> <p>IMPORTANT: the participant-level directory names should match participant_ids in the mr_proc_manifest.csv. It is recommended to use participant_id naming format to exclude any non-alphanumeric chacaters (e.g. \"-\" or \"_\"). If your participant_id does contain these characters, it is still recommended to remove them from the participant-level DICOM directory names (e.g., QPN_001 --&gt; QPN001).  </p> <p>Note</p> <p>It is okay for the participant directory to have messy internal subdir tree with DICOMs from multiple modalities. (See data org schematic for details). The run script will search and validate all available DICOM files automatically. </p> <ul> <li>Run run_dicom_org.py to:<ul> <li>Search: Find all the DICOMs inside the participant directory. </li> <li>Validate: Excludes certain individual dicom files that are invalid or contain scanner-derived data not compatible with BIDS conversion.</li> <li>Copy or Symlink (Recommended): Creates symlinks from <code>raw_dicoms/</code> to the <code>&lt;DATASET_ROOT&gt;/dicom</code>, where all participant specific dicoms are in a flat list.</li> </ul> </li> </ul> <p>Sample cmd: <pre><code>python run_dicom_org.py \\\n    --global_config &lt;global_config_file&gt; \\\n    --session_id &lt;session_id&gt; \\\n    --use_symlinks \n</code></pre></p>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/","title":"fmriprep","text":""},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#objective","title":"Objective","text":"<p>Run fMRIPrep pipeline on BIDS formatted dataset. Note that a standard fMRIPrep run also include FreeSurfer processing.</p>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/bids</code></li> <li><code>&lt;DATASET_ROOT&gt;/derivatives/fmriprep/</code></li> <li><code>&lt;DATASET_ROOT&gt;/derivatives/freesurfer/</code></li> <li><code>bids_filter.json</code></li> </ul>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#procedure","title":"Procedure","text":"<ul> <li>Ensure you have the appropriate fMRIPrep container listed in your <code>global_configs.json</code> </li> <li>Use run_fmriprep.py script to run fmriprep pipeline. </li> <li>You can run \"anatomical only\" workflow by adding <code>--anat_only</code> flag</li> <li>(Optional) Copy+Rename sample_bids_filter.json to <code>bids_filter.json</code> in the code repo itself. Then edit <code>bids_filter.json</code> to filter certain modalities / acquisitions. This is common when you have multiple T1w acquisitions (e.g. Neuromelanin, SPIR etc.) for a given modality. </li> </ul> <p>Note</p> <p>When <code>--use_bids_filter</code> flag is set, this <code>bids_filter.json</code> is automatically copied into <code>&lt;DATASET_ROOT&gt;/bids/bids_filter.json</code> to be seen by the Singularity container.</p> <ul> <li>For FreeSurfer tasks, you need to have a license.txt file inside <code>&lt;DATASET_ROOT&gt;/derivatives/freesurfer/</code></li> <li>fMRIPrep manages brain-template spaces using TemplateFlow. These templates can be shared across studies and datasets. Use <code>global_configs.json</code> to specify path to <code>TEMPLATEFLOW_DIR</code> where these templates can reside. </li> </ul> <p>Note</p> <p>For machines with Internet connections, all required templates are automatically downloaded duing the fMRIPrep run.</p> <p>Sample cmd: <pre><code>python run_fmriprep.py \\\n--global_config &lt;global_config_file&gt; \\\n--participant_id MNI01 \\\n--session_id 01 \\\n--use_bids_filter </code></pre></p> <p>Note</p> <p>Unlike DICOM and BIDS run scripts, <code>run_fmriprep.py</code> can only process 1 participant at a time due to heavy compute requirements of fMRIPrep. For parallel processing on a cluster, sample HPC job scripts (slurm and sge) are provided in hpc subdir. </p> <p>Note</p> <p>You can change default run parameters in the run_fmriprep.sh by looking at the documentation</p> <p>Note</p> <p>Clean-up working dir (<code>fmriprep_wf</code>): fMRIPrep run generates huge number of intermediate files. You should remove those after successful run to free-up space.</p>"},{"location":"mr_proc/workflow/proc_pipe/fmriprep/#fmriprep-tasks","title":"fMRIPrep tasks","text":"<ul> <li>Main MR processing tasks run by fmriprep (see fMRIPrep for details):<ul> <li>Preprocessing<ul> <li>Bias correction / Intensity normalization (N4)</li> <li>Brain extraction (ANTs)</li> <li>Spatial normalization to standard space(s)</li> </ul> </li> <li>Anatomical<ul> <li>Tissue segmentation (FAST)</li> <li>FreeSurfer recon-all</li> </ul> </li> <li>Functional<ul> <li>BOLD reference image estimation</li> <li>Head-motion estimation</li> <li>Slice time correction</li> <li>Susceptibility Distortion Correction (SDC)</li> <li>Pre-processed BOLD in native space</li> <li>EPI to T1w registration</li> <li>Resampling BOLD runs onto standard spaces</li> <li>EPI sampled to FreeSurfer surfaces</li> <li>Confounds estimation</li> <li>ICA-AROMA (not run by default)</li> </ul> </li> <li>Qualtiy Control<ul> <li>Visual reports</li> </ul> </li> </ul> </li> </ul>"},{"location":"mr_proc/workflow/proc_pipe/mriqc/","title":"mriqc","text":""},{"location":"mr_proc/workflow/proc_pipe/mriqc/#mriqc-image-processing-pipeline","title":"MRIQC image processing pipeline","text":"<p>MRIQC processes the participants and produces image quality metrics from T1w, T2w and BOLD data.</p>"},{"location":"mr_proc/workflow/proc_pipe/mriqc/#mriqc","title":"MRIQC","text":"<ul> <li>Ensure you are running the appropriate MRIQC image, as provided here</li> <li>Use run_mriqc.py to run MRIQC pipeline directly or wrap the script in an SGE/Slurm script to run on cluster</li> </ul> <p>python run_mriqc.py --global_config CONFIG.JSON --subject_id 001 --output_dir OUTPUT_DIR_PATH     - Mandatory: Pass in the absolute path to the configuration containing the MRIQC container and data directory to <code>global_config</code>     - Mandatory: Pass in the subject id to <code>participant_id</code>     - Mandatory: Pass in the subject id to <code>session_id</code>     - Mandatory: Pass in the absolute path to the output directory to <code>output_dir</code></p> <p>Note</p> <p>An example config is located here</p> <p>Sample cmd: <pre><code>python run_mriqc.py \\\n--global_config GLOBAL_CONFIG \\\n--participant_id SUBJECT_ID \\\n--output_dir OUTPUT_DIR \\\n--session_id SESSION_ID\n</code></pre></p> <p>Note</p> <p>A run for a participant is considered successful when the participant's log file reads <code>Participant level finished successfully</code></p>"},{"location":"mr_proc/workflow/proc_pipe/mriqc/#evaluate-mriqc-results","title":"Evaluate MRIQC Results","text":"<ul> <li>Use mriqc_tracker.py to determine how many subjects successfully passed through the MRIQC pipeline<ul> <li>Mandatory: Pass in the subject directory as an argument</li> </ul> </li> <li>After a successful run of the script, a dictionary called tracker_configs is returned contained whether the subject passed through the pipeline successfully</li> </ul> <p>Note</p> <p>Multiple sessions can be evaluated, but each session will require a new job running this script</p> <p>Sample cmd: <pre><code>&gt;&gt;&gt; results = {\"pipeline_complete': mriqc_tracker.eval_mriqc(subject_dir, session_id)}\n&gt;&gt;&gt; results\n SUCCESS\n&gt;&gt;&gt; results = {\"MRIQC_BOLD': mriqc_tracker.check_bold(subject_dir, session_id)}\n&gt;&gt;&gt; results\n FAIL\n</code></pre></p>"}]}